{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet33\n"
     ]
    }
   ],
   "source": [
    "# Resnet33\n",
    "print('Resnet33')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248265.51371000003\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "#input\n",
    "a0 = np.random.randn(N, D_in) #Sizes are other way round\n",
    "#output\n",
    "a2 = np.random.randn(N, D_out)\n",
    "#weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "tic = time.process_time()\n",
    "\n",
    "for i in range(0, 500):\n",
    "#Forward Pass\n",
    "    z = np.dot(a0, w1)\n",
    "    a1 = np.maximum(z, 0) #ReLU\n",
    "    a2_hat = np.dot(a1, w2)\n",
    "\n",
    "    #Backward Pass\n",
    "    #Loss function is mean squared error\n",
    "    loss = np.square(a2 - a2_hat)\n",
    "    loss = np.sum(loss, axis = 1, keepdims = True)\n",
    "    #if(i%100 == 99):\n",
    "    #    print(loss)\n",
    "\n",
    "    #Calculate dw1 and dw2\n",
    "    #dw1 = 1/N*dw1\n",
    "    da2 = -2*(a2-a2_hat)\n",
    "    dw2 = np.dot(a1.T, da2)\n",
    "    #print(dw2.shape)\n",
    "    da1 = np.dot(da2, w2.T)\n",
    "    #print(dwa1.shape)\n",
    "    dz1 = np.maximum(da1, 0)\n",
    "    #print(dz1.shape)\n",
    "    dw1 = np.dot(a0.T, dz1)\n",
    "    #print(dw1.shape)\n",
    "\n",
    "    w1 = w1 - learning_rate*dw1\n",
    "    w2 = w2 - learning_rate*dw2\n",
    "    \n",
    "    \n",
    "toc = time.process_time()\n",
    "\n",
    "print(1000*(toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a simple 2 layer Neural Network written using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1000])\n",
      "torch.Size([64, 10])\n",
      "15212.888563000035\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "#Input\n",
    "a0=torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "print(a0.size())\n",
    "#Output\n",
    "a2=torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "print(a2.size())\n",
    "\n",
    "#Weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "tic = time.process_time()\n",
    "#print(type(a0))\n",
    "for i in range(0,500):\n",
    "    #forward\n",
    "    z1 = torch.mm(a0,w1)\n",
    "    a1 = torch.clamp(z1, min=0)\n",
    "    #print(a1.size())\n",
    "    a2_hat = torch.mm(a1,w2)\n",
    "    #print(a2_hat.size())\n",
    "\n",
    "    loss = torch.sum(torch.pow((a2 - a2_hat),2),1)#Square + sum of rows\n",
    "    #print((a2 - a2_hat).size())\n",
    "\n",
    "    #Backprop\n",
    "    da2 = -2*(a2 - a2_hat)\n",
    "    dw2 = torch.mm(a1.transpose(0,1), da2)\n",
    "    da1 = torch.mm(da2, w2.transpose(0,1))\n",
    "    dz1 = torch.clamp(da1,min=0)\n",
    "    dw1 = torch.mm(a0.transpose(0,1), dz1)\n",
    "\n",
    "    w1 = w1 - learning_rate*dw1\n",
    "    w2 = w2 - learning_rate*dw2\n",
    "    \n",
    "toc = time.process_time()\n",
    "\n",
    "print(1000*(toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 tensor(395.5395, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "199 tensor(1.1924, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "299 tensor(0.0069, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "399 tensor(0.0002, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "499 tensor(3.7465e-05, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Use Autograd - automatic gradient calculations\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "a0 = torch.randn(N, D_in, dtype=dtype,device=device)\n",
    "a2 = torch.randn(N, D_out, dtype=dtype,device=device)\n",
    "\n",
    "w1 = torch.randn(D_in, H, dtype=dtype, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, dtype=dtype, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for i in range(0, 500):\n",
    "    \n",
    "    a2_hat = torch.mm(torch.clamp(torch.mm(a0,w1), min=0),w2) #forward pass\n",
    "\n",
    "    loss = torch.pow((a2 - a2_hat),2) #Cost function\n",
    "    loss = loss.sum()\n",
    "    if(i%100 == 99):\n",
    "        print(i, loss)\n",
    "\n",
    "    loss.backward() #Computes gradient of all tensors with requires_grad=True wrt scalar tensor 'loss'\n",
    "\n",
    "    with torch.no_grad(): #Temporary disables gradient calculation\n",
    "        w1 -= learning_rate*w1.grad #As a result only the value will change\n",
    "        w2 -= learning_rate*w2.grad #We don't want pytorch to calculate the new gradients\n",
    "        \n",
    "        w1.grad.zero_() #If not used can cause weird results\n",
    "        w2.grad.zero_() #Pytorch accumulates the gradients during backprop, otherwise gradient will point in another direction\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "<generator object Module.parameters at 0x7f6d745c5138>\n",
      "0 tensor(690.2531, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#Torch nn module\n",
    "import torch\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module): #Inherited class of torch.nn.Module class\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__() #needed\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(D_in, H) #sort of like function pointers\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, a0):\n",
    "        z1 = self.linear1(a0)\n",
    "        a1 = self.activation1(z1)\n",
    "        a2 = self.linear2(a1)\n",
    "        return a2\n",
    "\n",
    "#Define parameters of NN\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "\n",
    "#Input and output\n",
    "a0 = torch.randn(N, D_in, dtype=dtype, device=device)\n",
    "a2 = torch.randn(N, D_out, dtype=dtype, device=device)\n",
    "\n",
    "print(a0.size()[-1])\n",
    "\n",
    "#Learning Rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "#Instantiate NN\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "print(model.parameters())\n",
    "\n",
    "#Define loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum') #Indicates output will be summed. Output size is NxD_out. So it will be summed to get a single scalar\n",
    "\n",
    "#Define optimizer\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for i in range(0, 1):\n",
    "    \n",
    "    #Forward pass\n",
    "    a2_pred = model(a0)\n",
    "    \n",
    "    #Compute Loss\n",
    "    loss = loss_fn(a2_pred, a2)\n",
    "    \n",
    "    print(i, loss)\n",
    "        \n",
    "    #Set gradients to zero\n",
    "    #optimizer.zero_grad()\n",
    "    \n",
    "        \n",
    "    #Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    #Update weights\n",
    "    #optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate*param.grad\n",
    "    \n",
    "    model.zero_grad()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 1024, 1])\n",
      "torch.Size([128, 32, 1024, 1])\n",
      "96\n",
      "torch.Size([128, 32, 1024, 1])\n",
      "9248\n",
      "torch.Size([128, 32, 1024, 1])\n",
      "64\n",
      "torch.Size([128, 32, 512, 1])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Residual_Stack_sim\n",
    "import torch\n",
    "\n",
    "\n",
    "inp = torch.randn(128,2,1024,1)\n",
    "print(inp.size())\n",
    "\n",
    "m = torch.nn.Conv2d(in_channels=2,out_channels=32,kernel_size=(1,1)) #equivalent to padding = 'SAME'\n",
    "out1 = m(inp)\n",
    "print(out1.size())\n",
    "\n",
    "print(sum([param.nelement() for param in m.parameters()]))\n",
    "\n",
    "ru1 = torch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3), padding=(1,1))\n",
    "out11 = ru1(out1)\n",
    "print(out11.size())\n",
    "\n",
    "print(sum([param.nelement() for param in ru1.parameters()]))\n",
    "\n",
    "ru2 = torch.nn.BatchNorm2d(num_features=32)\n",
    "out12 = ru1(out11)\n",
    "print(out12.size())\n",
    "\n",
    "print(sum([param.nelement() for param in ru2.parameters()]))\n",
    "\n",
    "n = torch.nn.MaxPool2d(kernel_size=(2,1), stride=(2,1))\n",
    "out2 = n(out12)\n",
    "print(out2.size())\n",
    "\n",
    "print(sum([param.nelement() for param in n.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet33(\n",
      "  (rn33_rs1): residual_stack(\n",
      "    (rs_conv1): Conv1d(2, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (rs_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (rs_ru1): residual_unit(\n",
      "      (ru_conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act1): ReLU()\n",
      "      (ru_conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act2): ReLU()\n",
      "    )\n",
      "    (rs_ru2): residual_unit(\n",
      "      (ru_conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act1): ReLU()\n",
      "      (ru_conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act2): ReLU()\n",
      "    )\n",
      "    (rs_mp1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (rn33_rs2): residual_stack(\n",
      "    (rs_conv1): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (rs_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (rs_ru1): residual_unit(\n",
      "      (ru_conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act1): ReLU()\n",
      "      (ru_conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act2): ReLU()\n",
      "    )\n",
      "    (rs_ru2): residual_unit(\n",
      "      (ru_conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act1): ReLU()\n",
      "      (ru_conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act2): ReLU()\n",
      "    )\n",
      "    (rs_mp1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (rn33_rs3): residual_stack(\n",
      "    (rs_conv1): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (rs_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (rs_ru1): residual_unit(\n",
      "      (ru_conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act1): ReLU()\n",
      "      (ru_conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act2): ReLU()\n",
      "    )\n",
      "    (rs_ru2): residual_unit(\n",
      "      (ru_conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act1): ReLU()\n",
      "      (ru_conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act2): ReLU()\n",
      "    )\n",
      "    (rs_mp1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (rn33_rs4): residual_stack(\n",
      "    (rs_conv1): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (rs_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (rs_ru1): residual_unit(\n",
      "      (ru_conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act1): ReLU()\n",
      "      (ru_conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act2): ReLU()\n",
      "    )\n",
      "    (rs_ru2): residual_unit(\n",
      "      (ru_conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act1): ReLU()\n",
      "      (ru_conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act2): ReLU()\n",
      "    )\n",
      "    (rs_mp1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (rn33_rs5): residual_stack(\n",
      "    (rs_conv1): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (rs_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (rs_ru1): residual_unit(\n",
      "      (ru_conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act1): ReLU()\n",
      "      (ru_conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act2): ReLU()\n",
      "    )\n",
      "    (rs_ru2): residual_unit(\n",
      "      (ru_conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act1): ReLU()\n",
      "      (ru_conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act2): ReLU()\n",
      "    )\n",
      "    (rs_mp1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (rn33_rs6): residual_stack(\n",
      "    (rs_conv1): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    (rs_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (rs_ru1): residual_unit(\n",
      "      (ru_conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act1): ReLU()\n",
      "      (ru_conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act2): ReLU()\n",
      "    )\n",
      "    (rs_ru2): residual_unit(\n",
      "      (ru_conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act1): ReLU()\n",
      "      (ru_conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (ru_bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (ru_act2): ReLU()\n",
      "    )\n",
      "    (rs_mp1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flat): Flatten()\n",
      "  (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (selu1): SELU()\n",
      "  (alphadrop1): AlphaDropout(p=0.95, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (selu2): SELU()\n",
      "  (alphadrop2): AlphaDropout(p=0.95, inplace=False)\n",
      "  (fc3): Linear(in_features=128, out_features=24, bias=True)\n",
      "  (smx1): Softmax(dim=None)\n",
      ")\n",
      "inputtorch.Size([128, 2, 1024])\n",
      "torch.Size([128, 32, 512])\n",
      "torch.Size([128, 32, 256])\n",
      "torch.Size([128, 32, 128])\n",
      "torch.Size([128, 32, 64])\n",
      "torch.Size([128, 32, 32])\n",
      "torch.Size([128, 32, 16])\n",
      "torch.Size([128, 512])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 24])\n",
      "164952\n",
      "rn33_rs1.rs_conv1.weight                \ttorch.Size([32, 2, 1])        \t        64\n",
      "rn33_rs1.rs_ru1.ru_conv1.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs1.rs_ru1.ru_conv1.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs1.rs_ru1.ru_conv2.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs1.rs_ru1.ru_conv2.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs1.rs_ru2.ru_conv1.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs1.rs_ru2.ru_conv1.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs1.rs_ru2.ru_conv2.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs1.rs_ru2.ru_conv2.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs2.rs_conv1.weight                \ttorch.Size([32, 32, 1])       \t      1024\n",
      "rn33_rs2.rs_ru1.ru_conv1.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs2.rs_ru1.ru_conv1.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs2.rs_ru1.ru_conv2.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs2.rs_ru1.ru_conv2.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs2.rs_ru2.ru_conv1.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs2.rs_ru2.ru_conv1.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs2.rs_ru2.ru_conv2.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs2.rs_ru2.ru_conv2.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs3.rs_conv1.weight                \ttorch.Size([32, 32, 1])       \t      1024\n",
      "rn33_rs3.rs_ru1.ru_conv1.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs3.rs_ru1.ru_conv1.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs3.rs_ru1.ru_conv2.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs3.rs_ru1.ru_conv2.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs3.rs_ru2.ru_conv1.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs3.rs_ru2.ru_conv1.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs3.rs_ru2.ru_conv2.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs3.rs_ru2.ru_conv2.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs4.rs_conv1.weight                \ttorch.Size([32, 32, 1])       \t      1024\n",
      "rn33_rs4.rs_ru1.ru_conv1.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs4.rs_ru1.ru_conv1.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs4.rs_ru1.ru_conv2.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs4.rs_ru1.ru_conv2.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs4.rs_ru2.ru_conv1.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs4.rs_ru2.ru_conv1.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs4.rs_ru2.ru_conv2.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs4.rs_ru2.ru_conv2.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs5.rs_conv1.weight                \ttorch.Size([32, 32, 1])       \t      1024\n",
      "rn33_rs5.rs_ru1.ru_conv1.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs5.rs_ru1.ru_conv1.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs5.rs_ru1.ru_conv2.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs5.rs_ru1.ru_conv2.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs5.rs_ru2.ru_conv1.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs5.rs_ru2.ru_conv1.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs5.rs_ru2.ru_conv2.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs5.rs_ru2.ru_conv2.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs6.rs_conv1.weight                \ttorch.Size([32, 32, 1])       \t      1024\n",
      "rn33_rs6.rs_ru1.ru_conv1.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs6.rs_ru1.ru_conv1.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs6.rs_ru1.ru_conv2.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs6.rs_ru1.ru_conv2.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs6.rs_ru2.ru_conv1.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs6.rs_ru2.ru_conv1.bias           \ttorch.Size([32])              \t        32\n",
      "rn33_rs6.rs_ru2.ru_conv2.weight         \ttorch.Size([32, 32, 3])       \t      3072\n",
      "rn33_rs6.rs_ru2.ru_conv2.bias           \ttorch.Size([32])              \t        32\n",
      "fc1.weight                              \ttorch.Size([128, 512])        \t     65536\n",
      "fc1.bias                                \ttorch.Size([128])             \t       128\n",
      "fc2.weight                              \ttorch.Size([128, 128])        \t     16384\n",
      "fc2.bias                                \ttorch.Size([128])             \t       128\n",
      "fc3.weight                              \ttorch.Size([24, 128])         \t      3072\n",
      "fc3.bias                                \ttorch.Size([24])              \t        24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "#Resnet\n",
    "# y <-> cnn\n",
    "#need to check affine in BatchNorm1d\n",
    "\n",
    "import torch\n",
    "\n",
    "class residual_unit(torch.nn.Module):\n",
    "    def __init__(self, N, C, L, W, training=False):\n",
    "        super(residual_unit, self).__init__()\n",
    "        self.ru_conv1 = torch.nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, padding=1, bias=True)\n",
    "        self.ru_bn1 = torch.nn.BatchNorm1d(32, affine=training)\n",
    "        self.ru_act1 = torch.nn.ReLU()\n",
    "        self.ru_conv2 = torch.nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, padding=1, bias=True)\n",
    "        self.ru_bn2 = torch.nn.BatchNorm1d(32, affine=training)\n",
    "        self.ru_act2 = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.ru_conv1(x)\n",
    "        y = self.ru_bn1(y)\n",
    "        y = self.ru_act1(y)\n",
    "        y = self.ru_conv2(y)\n",
    "        y = self.ru_bn2(y)\n",
    "        y = y + x\n",
    "        y = self.ru_act2(y)\n",
    "        return y\n",
    "\n",
    "class residual_stack(torch.nn.Module):\n",
    "    def __init__(self, N, C, L, W, training=False):\n",
    "        super(residual_stack, self).__init__()\n",
    "        self.rs_conv1 = torch.nn.Conv1d(in_channels=C, out_channels=32, kernel_size=1, bias=False)\n",
    "        self.rs_bn1 = torch.nn.BatchNorm1d(32, affine=training)\n",
    "        self.rs_ru1 = residual_unit(N, C, L, W, training) #Create an object of the custom nn model\n",
    "        self.rs_ru2 = residual_unit(N, C, L, W, training)\n",
    "        self.rs_mp1 = torch.nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.rs_conv1(x)\n",
    "        y = self.rs_bn1(y)\n",
    "        y = self.rs_ru1(y)\n",
    "        y = self.rs_ru2(y)\n",
    "        y = self.rs_mp1(y)\n",
    "        return y\n",
    "    \n",
    "class resnet33(torch.nn.Module):\n",
    "    def __init__(self, N, C, L, W, training=False):\n",
    "        super(resnet33, self).__init__()\n",
    "        self.rn33_rs1 = residual_stack(N, 2, 1024, training) #output is N*32*512\n",
    "        self.rn33_rs2 = residual_stack(N, 32, 512, training) #output is N*32*256\n",
    "        self.rn33_rs3 = residual_stack(N, 32, 256, training) #output is N*32*128\n",
    "        self.rn33_rs4 = residual_stack(N, 32, 128, training) #output is N*32*64\n",
    "        self.rn33_rs5 = residual_stack(N, 32, 64, training) #output is N*32*32\n",
    "        self.rn33_rs6 = residual_stack(N, 32, 32, training) #output is N*32*16\n",
    "        self.flat = torch.nn.Flatten() #output is N*512\n",
    "        self.fc1 = torch.nn.Linear(512, 128) #output is N*128\n",
    "        self.selu1 = torch.nn.SELU()\n",
    "        self.alphadrop1 = torch.nn.AlphaDropout(p=0.95)\n",
    "        self.fc2 = torch.nn.Linear(128, 128) #output is N*128\n",
    "        self.selu2 = torch.nn.SELU()\n",
    "        self.alphadrop2 = torch.nn.AlphaDropout(p=0.95)\n",
    "        self.fc3 = torch.nn.Linear(128, 24) #output is N*24\n",
    "        self.smx1 = torch.nn.Softmax()#dimension\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('input' + str(x.size()))\n",
    "        y = self.rn33_rs1(x)\n",
    "        print(y.size())\n",
    "        y = self.rn33_rs2(y)\n",
    "        print(y.size())\n",
    "        y = self.rn33_rs3(y)\n",
    "        print(y.size())\n",
    "        y = self.rn33_rs4(y)\n",
    "        print(y.size())\n",
    "        y = self.rn33_rs5(y)\n",
    "        print(y.size())\n",
    "        y = self.rn33_rs6(y)\n",
    "        print(y.size())\n",
    "        #85272 parameters\n",
    "        y = self.flat(y)\n",
    "        print(y.size())\n",
    "        y = self.fc1(y)\n",
    "        y = self.selu1(y)\n",
    "        y = self.alphadrop1(y)\n",
    "        print(y.size())\n",
    "        y = self.fc2(y)\n",
    "        y = self.selu2(y)\n",
    "        y = self.alphadrop2(y)\n",
    "        print(y.size())\n",
    "        y = self.fc3(y)\n",
    "        y = self.smx1(y)\n",
    "        print(y.size())\n",
    "        return y\n",
    "        \n",
    "\n",
    "#Initialization\n",
    "N, C, L, W, modulation_classes = 128,2,1024,1,24\n",
    "x = torch.randn(N,C,L)\n",
    "y = torch.randn(N, modulation_classes)\n",
    "training = True\n",
    "learning_rate =  1e-4\n",
    "\n",
    "#Instantiate Model\n",
    "model = resnet33(N, C, L, W, training)\n",
    "\n",
    "#Print Model for reference\n",
    "print(model)\n",
    "\n",
    "y_pred = model(x)\n",
    "\n",
    "#Print number of parameters\n",
    "print(sum([param.nelement() for param in model.parameters()]))\n",
    "    \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print('{:s}\\t{:s}\\t{:s}'.format(name.ljust(40), str(param.size()).ljust(30), str(param.nelement()).rjust(10)))\n",
    "\n",
    "#Define Loss function\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "#Define optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "if(False):\n",
    "    for i in range(0, 500):\n",
    "        #Calculate predicted values\n",
    "        y_pred = model(x)\n",
    "\n",
    "        #Calculate loss\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        #print(i, loss)\n",
    "        if(i%100 == 99):\n",
    "            print(i, loss)\n",
    "\n",
    "        #Set gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Backprop\n",
    "        loss.backward()\n",
    "\n",
    "        #Update parameters\n",
    "        optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
